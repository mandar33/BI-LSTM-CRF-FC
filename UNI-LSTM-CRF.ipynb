{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandar33/BI-LSTM-CRF-FC/blob/main/BI-LSTM-CRF-FC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BQoZMdHRfW_x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j_SZR0X6LUoV"
      },
      "outputs": [],
      "source": [
        "class dictionary():\n",
        "    def __init__(self):\n",
        "        self.word_freq={}\n",
        "        self.id2word={}\n",
        "        self.word2id={}\n",
        "    \n",
        "    def add_word(self,word):\n",
        "        if word in self.word_freq:\n",
        "            self.word_freq[word]+=1\n",
        "        else:\n",
        "            self.word_freq[word]=1\n",
        "        \n",
        "    def create_mapping(self):\n",
        "        self.word_freq['[PAD]']=1000001\n",
        "        self.word_freq['[UNK]']=1000000\n",
        "        c_unk=0\n",
        "        dic_items=[]\n",
        "        for k in self.word_freq.keys():\n",
        "            if self.word_freq[k]>1 or np.random.uniform()>0.5:\n",
        "                dic_items.append((k,self.word_freq[k]))\n",
        "            else:\n",
        "                c_unk+=1\n",
        "        ordered_lis=sorted( dic_items, key=lambda x: (-x[1],x[0]))\n",
        "        assert ordered_lis[0][0]=='[PAD]'\n",
        "        self.id2word=dict([(i,ordered_lis[i][0]) for i in range(len(ordered_lis))])\n",
        "        self.word2id=dict([(ordered_lis[i][0],i) for i in range(len(ordered_lis))])\n",
        "        self.ordered_lis=ordered_lis\n",
        "        return c_unk\n",
        "\n",
        "    def get_id(self,word):\n",
        "        if word in self.word2id:\n",
        "            return self.word2id[word]\n",
        "        else:\n",
        "            return 1\n",
        "    \n",
        "    def get_word(self,idx):\n",
        "        return self.id2word[idx]\n",
        "    \n",
        "    def get_len(self):\n",
        "        return len(self.id2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbxxflzpUrhn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CmEiiZzvLZ3r"
      },
      "outputs": [],
      "source": [
        "class I2B2DatasetReader(Dataset):\n",
        "    def __init__(self,data_path,dic_word,dic_char,training=False):\n",
        "        super(I2B2DatasetReader,self).__init__()\n",
        "        #read data -> X:[[word,word,word,...],[...],[...]] Y:[[0,1,2,3...],...]\n",
        "        f=open(data_path,encoding='utf-8')\n",
        "        X=[[]]\n",
        "        Y=[[]]\n",
        "\n",
        "        line=f.readline()\n",
        "        self.label_map=[\"O\",\"B-problem\",\"I-problem\",\"B-test\",\"I-test\",\"B-treatment\",\"I-treatment\",\"B-MISC\",\"I-MISC\"]\n",
        "        self.label_num=len(self.label_map)\n",
        "\n",
        "        while line:\n",
        "            if line=='\\n':\n",
        "                if len(X[-1])>0:\n",
        "                    X.append([])\n",
        "                    Y.append([])\n",
        "            else:\n",
        "                word,ner=line.split()\n",
        "                assert ner in self.label_map\n",
        "                word=re.sub('\\d','0',word)      #replace all the digits with 0, this helps\n",
        "                X[-1].append(word)\n",
        "                Y[-1].append(self.label_map.index(ner))\n",
        "            line=f.readline()\n",
        "                \n",
        "        f.close()\n",
        "        if len(X[-1])==0:\n",
        "            X=X[:-1]\n",
        "            Y=Y[:-1]\n",
        "\n",
        "        self.label=Y\n",
        "        self.data_num=len(X)\n",
        "\n",
        "        #get word dictionary\n",
        "        if training:\n",
        "            dic_word=dictionary()\n",
        "            for sentence in X:\n",
        "                for word in sentence:\n",
        "                    dic_word.add_word(word)\n",
        "            dic_word.create_mapping()\n",
        "\n",
        "        #get word_ids: list of lists\n",
        "        #encode words str->id\n",
        "        self.word_ids=[]\n",
        "        for i in range(len(X)):\n",
        "            self.word_ids.append(list(map(lambda x:dic_word.get_id(x), X[i])))\n",
        "\n",
        "        #get character dictionary\n",
        "        if training:\n",
        "            dic_char=dictionary()\n",
        "            for sentence in X:\n",
        "                text=''.join(sentence)\n",
        "                for char in text:\n",
        "                    dic_char.add_word(char)\n",
        "            dic_char.create_mapping()\n",
        "\n",
        "        #get char_ids: list of lists of lists\n",
        "        self.char_ids=[]\n",
        "        for sentence in X:\n",
        "            s=[]\n",
        "            for word in sentence:\n",
        "                s.append(list(map(lambda x:dic_char.get_id(x), word)))\n",
        "            self.char_ids.append(s)\n",
        "\n",
        "        self.dic_word=dic_word\n",
        "        self.dic_char=dic_char\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.word_ids[index],self.char_ids[index],self.label[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_num\n",
        "     \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yY9jjDUBLfnj"
      },
      "outputs": [],
      "source": [
        "def expand_dic(dictionary,embedding_path,paths):\n",
        "            f=open(embedding_path,encoding=\"utf-8\")\n",
        "            line=f.readline()\n",
        "            word2emb={}\n",
        "            while line:\n",
        "                line=line.split()\n",
        "                word2emb[line[0]]=torch.from_numpy(np.array(line[1:],dtype=np.str).astype(np.float))\n",
        "                line=f.readline()\n",
        "\n",
        "            words=[]\n",
        "            for data_path in paths:\n",
        "                f=open(data_path,encoding='utf-8')\n",
        "                line=f.readline()\n",
        "                while line:\n",
        "                    if line=='\\n':\n",
        "                        pass\n",
        "                    else:\n",
        "                        word=line.split()[0]\n",
        "                        words.append(word)\n",
        "                    line=f.readline()\n",
        "                f.close()\n",
        "\n",
        "            train_len=dictionary.get_len()\n",
        "            for word in words:\n",
        "                if word not in dictionary.word2id and any([x in word2emb for x in [word,word.lower(),re.sub('\\d','0',word.lower())]]):\n",
        "                    dictionary.word2id[word]=dictionary.get_len()\n",
        "                    dictionary.id2word[dictionary.get_len()]=word\n",
        "                    dictionary.ordered_lis.append((word,0))\n",
        "            num_add=dictionary.get_len()-train_len\n",
        "            print(\"original word num: %d  expand num: %d\"%(train_len,num_add)) \n",
        "            return dictionary,word2emb\n",
        "\n",
        "def collate_batch(batch):\n",
        "            #input is a list of tuples\n",
        "            word_num=list(map(lambda x:len(x[0]),batch))\n",
        "            max_word_num=max(word_num)\n",
        "            word_ids=list(map(lambda x:x[0]+[0]*(max_word_num-len(x[0])),batch))\n",
        "            label_ids=list(map(lambda x:x[2]+[0]*(max_word_num-len(x[2])),batch))\n",
        "\n",
        "            max_word_length=max(list(map(lambda x:max([len(i) for i in x[1]]),batch)))\n",
        "            char_ids=[]\n",
        "            for tuple in batch:\n",
        "                s=[]\n",
        "                for word in tuple[1]:\n",
        "                    s.append(word+[0]*(max_word_length-len(word)))\n",
        "                s=s+[[0]*max_word_length for i in range((max_word_num-len(s)))]\n",
        "                char_ids.append(s)\n",
        "\n",
        "            word_num=torch.LongTensor(word_num)\n",
        "            word_ids=torch.LongTensor(word_ids)\n",
        "            char_ids=torch.LongTensor(char_ids)\n",
        "            label_ids=torch.LongTensor(label_ids)\n",
        "\n",
        "            return word_num,word_ids,char_ids,label_ids\n",
        "\n",
        "def forward_alg(observation,transition,word_num):\n",
        "\n",
        "            def log_sum_exp(matrix,dim):\n",
        "                maximum,_=matrix.max(dim=dim,keepdim=True)  #to avoid NaN\n",
        "                return (maximum+torch.log(torch.exp(matrix-maximum).sum(dim=dim,keepdim=True))).squeeze(1)\n",
        "\n",
        "            observation=observation.transpose(1,2)\n",
        "            transition=transition.unsqueeze(0).expand(observation.size(0),-1,-1)\n",
        "            alpha=torch.zeros_like(observation)\n",
        "            alpha[:,:,0:1]=observation[:,:,0:1]\n",
        "            for i in range(1,observation.size(2)):\n",
        "                alpha[:,:,i:i+1]=(observation[:,:,i]+log_sum_exp(alpha[:,:,i-1:i]+transition,dim=1)).unsqueeze(2)\n",
        "            end_label=alpha[:,10,1:]     #(batch_size, sequence_len)\n",
        "            return end_label.gather(1,word_num.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "def list_batch(pred,word_num,word_ids,label_ids,dic_word,label_map):\n",
        "    pred=pred.tolist()\n",
        "    word_num=word_num.tolist()\n",
        "    label_ids=label_ids.tolist()\n",
        "    word_ids=word_ids.tolist()\n",
        "\n",
        "    outputs=[]\n",
        "    for i in range(len(word_num)):\n",
        "        seq_len=word_num[i]\n",
        "        prediction=pred[i][:seq_len]\n",
        "        target=label_ids[i][:seq_len]\n",
        "        words=word_ids[i][:seq_len]\n",
        "        prediction=list(map(lambda x: label_map[x], prediction))\n",
        "        target=list(map(lambda x: label_map[x], target))\n",
        "        words=list(map(lambda x: dic_word.get_word(x), words))\n",
        "        for j in range(seq_len):\n",
        "            outputs.append(' '.join([words[j],target[j],prediction[j]]))\n",
        "        outputs.append('')\n",
        "    \n",
        "    return outputs  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2pxPf4PxaxaA"
      },
      "outputs": [],
      "source": [
        "class LSTM_CRF(nn.Module):\n",
        "    def __init__(self,word2emb,dic_word,dic_char):\n",
        "        super(LSTM_CRF, self).__init__()\n",
        "        word_emb_dim=300\n",
        "        word_lstm_dim=300\n",
        "        char_emb_dim=25\n",
        "        char_lstm_dim=25\n",
        "        label_num=9\n",
        "        dropout_rate=0.5\n",
        "            \n",
        "        word_emb=nn.Embedding(dic_word.get_len(),word_emb_dim,padding_idx=0)\n",
        "        for i in range(dic_word.get_len()):\n",
        "            word=dic_word.ordered_lis[i][0]\n",
        "            if word in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[word]\n",
        "            elif word.lower() in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[word.lower()]\n",
        "            elif re.sub('\\d','0',word.lower()) in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[re.sub('\\d','0',word.lower())]\n",
        "        #print(word_emb.weight.data[0])\n",
        "\n",
        "\n",
        "        char_emb=nn.Embedding(dic_char.get_len(),char_emb_dim,padding_idx=0)\n",
        "\n",
        "        self.char_emb=char_emb\n",
        "        self.char_lstm=nn.LSTM(char_emb_dim,char_lstm_dim,batch_first=True,num_layers = 1,bidirectional=False)\n",
        "        self.word_emb=word_emb\n",
        "        self.dropout=nn.Dropout(dropout_rate)\n",
        "        self.word_lstm=nn.LSTM(word_emb_dim+char_lstm_dim, word_lstm_dim,batch_first=True,num_layers = 1,bidirectional=False)\n",
        "        self.hidden2out = nn.Linear(word_lstm_dim, label_num)\n",
        "        #self.softmax = nn.LogSoftmax()\n",
        "        # self.fc=nn.Sequential(\n",
        "        #      nn.Linear(word_lstm_dim*2,word_lstm_dim),\n",
        "        #      nn.Relu(),\n",
        "        #      nn.Linear(word_lstm_dim,label_num)\n",
        "        # )\n",
        "        #self.fc=  nn.Linear(word_lstm_dim*2,label_num)\n",
        "        self.transition=nn.Parameter(torch.full((label_num+2,label_num+2),math.log(1/label_num)))\n",
        "        \n",
        "        self.char_lstm_dim=char_lstm_dim\n",
        "        self.char_emb_dim=char_emb_dim\n",
        "        self.word_lstm_dim=word_lstm_dim\n",
        "        self.word_emb_dim=word_emb_dim\n",
        "        self.label_num=label_num\n",
        "        \n",
        "    def get_feature(self,word_num,word_ids,char_ids):\n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        char_input=self.char_emb(char_ids)\n",
        "        #print(char_input.size())    #4 dimensional\n",
        "        char_emb_dim=char_input.size(3)\n",
        "        word_len=char_input.size(2)\n",
        "        char_input=char_input.view(batch_size*sequence_len,word_len,char_emb_dim)\n",
        "        char_hidden,_=self.char_lstm(char_input)    #second output \"_\" is equal to char_output below\n",
        "        forward_=char_hidden[:,-1,:self.char_lstm_dim]\n",
        "        backward_=char_hidden[:,0,self.char_lstm_dim:]\n",
        "        char_output=torch.cat((forward_,backward_),dim=-1)\n",
        "        char_output=char_output.view(batch_size,sequence_len,self.char_lstm_dim)\n",
        "\n",
        "        index=torch.LongTensor(list(range(sequence_len))).cuda().unsqueeze(0).expand(batch_size,sequence_len)\n",
        "        condition=word_num.unsqueeze(1).expand(batch_size,sequence_len)>index\n",
        "        mask=torch.where(condition,torch.ones(1,).cuda(),torch.zeros(1,).cuda()).unsqueeze(2)\n",
        "        char_output*=mask   #to mask all the padding tokens\n",
        "\n",
        "        word_feature=self.word_emb(word_ids)\n",
        "        word_feature=torch.cat((word_feature,char_output),dim=-1)\n",
        "        word_feature=self.dropout(word_feature)\n",
        "        word_feature,_=self.word_lstm(word_feature)\n",
        "\n",
        "        word_feature = self.hidden2out(word_feature)\n",
        "        #word_feature = self.softmax(word_feature)\n",
        "\n",
        "        #word_feature=self.hidden(word_feature)\n",
        "        #word_feature = self.fc(word_feature)\n",
        "        \n",
        "        word_feature*=mask\n",
        "      \n",
        "        return word_feature,mask\n",
        "    \n",
        "    def forward(self,word_num,word_ids,char_ids,label_ids):    \n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        word_feature,mask=self.get_feature(word_num,word_ids,char_ids)\n",
        "        \n",
        "        #compute numerator: the score of target label sequence\n",
        "        numerator=word_feature.gather(2,label_ids.unsqueeze(2)).squeeze(2).sum(dim=1)\n",
        "        #print(numerator.size())\n",
        "        padded_label=torch.cat((torch.full((batch_size,1),9,dtype=torch.long).cuda(), label_ids), dim=1)\n",
        "\n",
        "        #print(self.transition[(padded_label[:,:-1],padded_label[:,1:])].size()) \n",
        "        #a tensor can be indexed by several LongTensors or lists, each of them corresponds with an axis\n",
        "        trans_score=self.transition[(padded_label[:,:-1],padded_label[:,1:])]   #size(batch_size,sequence_len)\n",
        "        trans_score*=mask.squeeze(2)\n",
        "        numerator+=trans_score.sum(dim=1)\n",
        "        last_label=(padded_label.gather(1,word_num.unsqueeze(1))).squeeze()\n",
        "        numerator+=self.transition[(last_label,torch.full((batch_size,),10,dtype=torch.long).cuda())]\n",
        "        \n",
        "        #prepare observation matrix\n",
        "        small=-1000\n",
        "        se_label=torch.full((batch_size,sequence_len,2),small,dtype=torch.float).cuda()*mask\n",
        "        observation=torch.cat((word_feature,se_label),dim=2)\n",
        "        observation=torch.cat((torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda(),\n",
        "                                observation,\n",
        "                                torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda()),dim=1)\n",
        "        \n",
        "       \n",
        "      \n",
        "\n",
        "        observation[:,0,9]=0\n",
        "        observation[:,-1,10]=0\n",
        "\n",
        "        denominator=forward_alg(observation,self.transition,word_num)   #the score of all the label sequences\n",
        "        loss=-(numerator-denominator)\n",
        "\n",
        "        return torch.mean(loss)\n",
        "    \n",
        "    def decode(self,word_num,word_ids,char_ids):\n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        word_feature,mask=self.get_feature(word_num,word_ids,char_ids)\n",
        "\n",
        "        index=torch.LongTensor(list(range(sequence_len))).cuda().unsqueeze(0).expand(batch_size,sequence_len)\n",
        "        condition=word_num.unsqueeze(1).expand(batch_size,sequence_len)==index\n",
        "        end_mask=torch.where(condition,torch.ones(1,).cuda(),torch.zeros(1,).cuda()).unsqueeze(2)\n",
        "\n",
        "        small=-1000\n",
        "        constrain=torch.full((batch_size,sequence_len,self.label_num+2),small,dtype=torch.float).cuda()*end_mask\n",
        "        constrain[:,:,10]=0         #tensor \"constrain\" is used to make sure all the paths finish at [END] state\n",
        "\n",
        "        se_label=torch.full((batch_size,sequence_len,2),small,dtype=torch.float).cuda()*mask    #correspond with Start and End label\n",
        "        observation=torch.cat((word_feature,se_label),dim=2)\n",
        "        observation+=constrain\n",
        "        observation=torch.cat((torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda(),\n",
        "                                observation,\n",
        "                                torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda()),dim=1)\n",
        "        observation[:,0,9]=0\n",
        "        observation[:,-1,10]=0\n",
        "        \n",
        "        #viterbi\n",
        "        observation=observation.transpose(1,2)\n",
        "        path=torch.zeros_like(observation).long().cuda()\n",
        "        transition=self.transition.unsqueeze(0).expand(batch_size,-1,-1)\n",
        "        z=observation[:,:,0:1]\n",
        "        for i in range(1,observation.size(2)):\n",
        "            values,indices=(z+transition).max(dim=1)\n",
        "            path[:,:,i]=indices\n",
        "            values+=observation[:,:,i]\n",
        "            z=values.unsqueeze(2)\n",
        "\n",
        "        last=path[:,10,-1:]\n",
        "        pred=last\n",
        "        for i in range(path.size(2)-2,1,-1):\n",
        "            last=path[:,:,i].gather(1,last)\n",
        "            pred=torch.cat((last,pred),dim=1)   \n",
        "        \n",
        "        #pred size: batch_size,sequence_len\n",
        "        #print(pred.size())\n",
        "\n",
        "        #validation     this step is unnecessary, just make sure there is nothing wrong\n",
        "        pred_=torch.cat((pred,torch.full((batch_size,1),10,dtype=torch.long).cuda()),dim=1)\n",
        "        condition=pred_.gather(1,word_num.unsqueeze(1)).squeeze(1)==10\n",
        "        assert condition.size(0)==(condition.sum().item())\n",
        "        #print(\"validation passed\")\n",
        "\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GljQpo2xMIMt",
        "outputId": "8c141b85-7c3e-4d39-bf88-27e3ebd43c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conlleval\n",
            "  Downloading conlleval-0.2-py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: conlleval\n",
            "Successfully installed conlleval-0.2\n"
          ]
        }
      ],
      "source": [
        "pip install conlleval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNTctkV9UudH",
        "outputId": "76a51edd-72d4-40f7-d3e3-0f7a038935f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original word num: 3868  expand num: 203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 61%|██████    | 60/98 [00:16<00:10,  3.55it/s]\n",
            "\n",
            "  1%|          | 1/98 [00:00<00:23,  4.11it/s]\u001b[A\n",
            "  4%|▍         | 4/98 [00:00<00:07, 12.22it/s]\u001b[A\n",
            "  7%|▋         | 7/98 [00:00<00:05, 17.31it/s]\u001b[A\n",
            " 10%|█         | 10/98 [00:00<00:04, 17.93it/s]\u001b[A\n",
            " 14%|█▍        | 14/98 [00:00<00:03, 22.59it/s]\u001b[A\n",
            " 18%|█▊        | 18/98 [00:00<00:03, 26.21it/s]\u001b[A\n",
            " 21%|██▏       | 21/98 [00:01<00:03, 22.94it/s]\u001b[A\n",
            " 26%|██▌       | 25/98 [00:01<00:02, 26.28it/s]\u001b[A\n",
            " 29%|██▊       | 28/98 [00:01<00:02, 26.52it/s]\u001b[A\n",
            " 33%|███▎      | 32/98 [00:01<00:02, 28.80it/s]\u001b[A\n",
            " 36%|███▌      | 35/98 [00:01<00:02, 29.11it/s]\u001b[A\n",
            " 40%|███▉      | 39/98 [00:01<00:01, 31.05it/s]\u001b[A\n",
            " 44%|████▍     | 43/98 [00:01<00:01, 31.08it/s]\u001b[A\n",
            " 49%|████▉     | 48/98 [00:01<00:01, 34.66it/s]\u001b[A\n",
            " 53%|█████▎    | 52/98 [00:01<00:01, 32.40it/s]\u001b[A\n",
            " 57%|█████▋    | 56/98 [00:02<00:01, 32.01it/s]\u001b[A\n",
            " 61%|██████    | 60/98 [00:02<00:01, 30.57it/s]\u001b[A\n",
            " 65%|██████▌   | 64/98 [00:02<00:01, 29.60it/s]\u001b[A\n",
            " 68%|██████▊   | 67/98 [00:02<00:01, 24.29it/s]\u001b[A\n",
            " 71%|███████▏  | 70/98 [00:02<00:01, 20.84it/s]\u001b[A\n",
            " 76%|███████▌  | 74/98 [00:02<00:01, 23.67it/s]\u001b[A\n",
            " 79%|███████▊  | 77/98 [00:03<00:00, 24.31it/s]\u001b[A\n",
            " 82%|████████▏ | 80/98 [00:03<00:00, 25.25it/s]\u001b[A\n",
            " 86%|████████▌ | 84/98 [00:03<00:00, 24.75it/s]\u001b[A\n",
            " 90%|████████▉ | 88/98 [00:03<00:00, 27.88it/s]\u001b[A\n",
            " 94%|█████████▍| 92/98 [00:03<00:00, 30.42it/s]\u001b[A\n",
            "100%|██████████| 98/98 [00:03<00:00, 25.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:  mean loss: 8.6811  f1 score: 10.26  best: 10.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1:  mean loss: 5.6218  f1 score: 21.61  best: 21.61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2:  mean loss: 4.4210  f1 score: 31.26  best: 31.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3:  mean loss: 3.6992  f1 score: 40.30  best: 40.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4:  mean loss: 3.0910  f1 score: 41.33  best: 41.33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5:  mean loss: 2.6475  f1 score: 47.47  best: 47.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6:  mean loss: 2.2996  f1 score: 47.79  best: 47.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7:  mean loss: 2.0189  f1 score: 48.67  best: 48.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8:  mean loss: 1.7920  f1 score: 47.74  best: 48.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9:  mean loss: 1.5883  f1 score: 51.40  best: 51.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10:  mean loss: 1.3568  f1 score: 50.36  best: 51.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 11:  mean loss: 1.2223  f1 score: 51.65  best: 51.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 12:  mean loss: 1.0691  f1 score: 52.04  best: 52.04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 13:  mean loss: 0.9767  f1 score: 51.93  best: 52.04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 14:  mean loss: 0.8573  f1 score: 53.79  best: 53.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 15:  mean loss: 0.7717  f1 score: 51.50  best: 53.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16:  mean loss: 0.6936  f1 score: 53.74  best: 53.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 17:  mean loss: 0.6516  f1 score: 53.33  best: 53.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 18:  mean loss: 0.6186  f1 score: 54.17  best: 54.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 19:  mean loss: 0.5936  f1 score: 52.77  best: 54.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 20:  mean loss: 0.5233  f1 score: 53.90  best: 54.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 21:  mean loss: 0.4797  f1 score: 50.92  best: 54.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 22:  mean loss: 0.4481  f1 score: 55.22  best: 55.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 23:  mean loss: 0.4324  f1 score: 53.97  best: 55.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 24:  mean loss: 0.4106  f1 score: 55.03  best: 55.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 25:  mean loss: 0.3569  f1 score: 55.79  best: 55.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 26:  mean loss: 0.3391  f1 score: 57.34  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 27:  mean loss: 0.3218  f1 score: 56.32  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 28:  mean loss: 0.3042  f1 score: 54.85  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 29:  mean loss: 0.3109  f1 score: 55.34  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 30:  mean loss: 0.2777  f1 score: 54.61  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 31:  mean loss: 0.2827  f1 score: 50.18  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 32:  mean loss: 0.2687  f1 score: 52.58  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 33:  mean loss: 0.2424  f1 score: 55.44  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 29.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 34:  mean loss: 0.2435  f1 score: 52.47  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 35:  mean loss: 0.2258  f1 score: 54.67  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 36:  mean loss: 0.2198  f1 score: 55.22  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 29.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 37:  mean loss: 0.2087  f1 score: 54.58  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 38:  mean loss: 0.2087  f1 score: 54.06  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 39:  mean loss: 0.1943  f1 score: 55.59  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 40:  mean loss: 0.1977  f1 score: 55.93  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 41:  mean loss: 0.1977  f1 score: 55.25  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 42:  mean loss: 0.1982  f1 score: 55.04  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 43:  mean loss: 0.1946  f1 score: 54.07  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 44:  mean loss: 0.1960  f1 score: 54.29  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 45:  mean loss: 0.1764  f1 score: 55.44  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 46:  mean loss: 0.1599  f1 score: 55.36  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 47:  mean loss: 0.1599  f1 score: 54.04  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 48:  mean loss: 0.1556  f1 score: 55.16  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 49:  mean loss: 0.1509  f1 score: 55.24  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 50:  mean loss: 0.1502  f1 score: 53.75  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 51:  mean loss: 0.1407  f1 score: 54.20  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 26.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 52:  mean loss: 0.1421  f1 score: 54.80  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 53:  mean loss: 0.1669  f1 score: 54.39  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 54:  mean loss: 0.1416  f1 score: 54.82  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 55:  mean loss: 0.1362  f1 score: 53.22  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 56:  mean loss: 0.1398  f1 score: 54.39  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 27.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 57:  mean loss: 0.1270  f1 score: 54.33  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 58:  mean loss: 0.1276  f1 score: 56.70  best: 57.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 98/98 [00:03<00:00, 28.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 59:  mean loss: 0.1236  f1 score: 57.39  best: 57.39\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE=32 \n",
        "LR=0.001\n",
        "CLIP=5.\n",
        "\n",
        "train_dataset=I2B2DatasetReader('./datafiles/train.txt',dictionary(),dictionary(),training=True)\n",
        "train_dataset.dic_word,word2emb=expand_dic(train_dataset.dic_word,\"datafiles/glove.6B.300d.txt\",['./datafiles/dev.txt','./datafiles/test.txt'])\n",
        "dev_dataset=I2B2DatasetReader('./datafiles/dev.txt',train_dataset.dic_word,train_dataset.dic_char)\n",
        "test_dataset=I2B2DatasetReader('./datafiles/test.txt',train_dataset.dic_word,train_dataset.dic_char)\n",
        "\n",
        "train_loader=DataLoader(train_dataset,BATCH_SIZE,shuffle=True,num_workers=2,collate_fn=collate_batch)\n",
        "dev_loader=DataLoader(dev_dataset,BATCH_SIZE,shuffle=False,num_workers=2,collate_fn=collate_batch)\n",
        "test_loader=DataLoader(test_dataset,BATCH_SIZE,shuffle=False,num_workers=2,collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "lstm_crf=LSTM_CRF(word2emb,train_dataset.dic_word,train_dataset.dic_char).cuda()\n",
        "optimizer=torch.optim.Adam(lstm_crf.parameters(),LR)\n",
        "\n",
        "best_score=0\n",
        "for epoch in range(60):\n",
        "    lstm_crf.train()\n",
        "    loss_lis=[]\n",
        "    pbar=tqdm(total=len(train_loader))\n",
        "    for i,(word_num,word_ids,char_ids,label_ids) in enumerate(train_loader):\n",
        "        loss=lstm_crf(word_num.cuda(),word_ids.cuda(),char_ids.cuda(),label_ids.cuda())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #nn.utils.clip_grad_norm_(lstm_crf.parameters(),CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_lis.append(loss.item())\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "    mean_loss=torch.mean(torch.tensor(loss_lis)).item()\n",
        "\n",
        "    lstm_crf.eval()\n",
        "    f1_score=0\n",
        "\n",
        "    #for loader in (dev_loader,test_loader):\n",
        "    loader=test_loader\n",
        "\n",
        "    outputs=[]\n",
        "    for i,(word_num,word_ids,char_ids,label_ids) in enumerate(loader):\n",
        "        word_num,word_ids,char_ids,label_ids=word_num.cuda(),word_ids.cuda(),char_ids.cuda(),label_ids.cuda()\n",
        "        pred=lstm_crf.decode(word_num,word_ids,char_ids)\n",
        "        outputs+=list_batch(pred,word_num,word_ids,label_ids, train_dataset.dic_word, train_dataset.label_map)\n",
        "\n",
        "    f=open('outputs.txt','w',encoding='utf-8')\n",
        "    f.write('\\n'.join(outputs))\n",
        "    f.close()\n",
        "    \n",
        "    os.system(\"python -m conlleval outputs.txt > results\")\n",
        "    #os.system(\"./conlleval < outputs.txt > results\")\n",
        "    f=open('results','r',encoding='utf-8')\n",
        "    f1_score=float(f.readlines()[1].split()[-1])\n",
        "    f.close()\n",
        "    best_score=max(best_score,f1_score)\n",
        "    \n",
        "    #print('epoch %d:  mean loss: %.4f  f1 score dev: %.2f  test: %.2f'%(epoch,mean_loss,f1_score[0],f1_score[1]))\n",
        "    print('epoch %d:  mean loss: %.4f  f1 score: %.2f  best: %.2f'%(epoch,mean_loss,f1_score,best_score))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CS598FIRSTDRAFTFinalProjet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOhKUN7eU7Uqar66CH0PQAP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
