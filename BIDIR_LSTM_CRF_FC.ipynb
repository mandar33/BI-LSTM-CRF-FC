{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandar33/BI-LSTM-CRF-FC/blob/main/BIDIR_LSTM_CRF_FC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BQoZMdHRfW_x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j_SZR0X6LUoV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class dictionary():\n",
        "    def __init__(self):\n",
        "        self.word_freq={}\n",
        "        self.id2word={}\n",
        "        self.word2id={}\n",
        "    \n",
        "    def add_word(self,word):\n",
        "        if word in self.word_freq:\n",
        "            self.word_freq[word]+=1\n",
        "        else:\n",
        "            self.word_freq[word]=1\n",
        "        \n",
        "    def create_mapping(self):\n",
        "        self.word_freq['[PAD]']=1000001\n",
        "        self.word_freq['[UNK]']=1000000\n",
        "        c_unk=0\n",
        "        dic_items=[]\n",
        "        for k in self.word_freq.keys():\n",
        "            if self.word_freq[k]>1 or np.random.uniform()>0.5:\n",
        "                dic_items.append((k,self.word_freq[k]))\n",
        "            else:\n",
        "                c_unk+=1\n",
        "        ordered_lis=sorted( dic_items, key=lambda x: (-x[1],x[0]))\n",
        "        assert ordered_lis[0][0]=='[PAD]'\n",
        "        self.id2word=dict([(i,ordered_lis[i][0]) for i in range(len(ordered_lis))])\n",
        "        self.word2id=dict([(ordered_lis[i][0],i) for i in range(len(ordered_lis))])\n",
        "        self.ordered_lis=ordered_lis\n",
        "        return c_unk\n",
        "\n",
        "    def get_id(self,word):\n",
        "        if word in self.word2id:\n",
        "            return self.word2id[word]\n",
        "        else:\n",
        "            return 1\n",
        "    \n",
        "    def get_word(self,idx):\n",
        "        return self.id2word[idx]\n",
        "    \n",
        "    def get_len(self):\n",
        "        return len(self.id2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wbxxflzpUrhn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CmEiiZzvLZ3r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class I2B2DatasetReader(Dataset):\n",
        "    def __init__(self,data_path,dic_word,dic_char,training=False):\n",
        "        super(I2B2DatasetReader,self).__init__()\n",
        "        #read data -> X:[[word,word,word,...],[...],[...]] Y:[[0,1,2,3...],...]\n",
        "        f=open(data_path,encoding='utf-8')\n",
        "        X=[[]]\n",
        "        Y=[[]]\n",
        "\n",
        "        line=f.readline()\n",
        "        self.label_map=[\"O\",\"B-problem\",\"I-problem\",\"B-test\",\"I-test\",\"B-treatment\",\"I-treatment\",\"B-MISC\",\"I-MISC\"]\n",
        "        self.label_num=len(self.label_map)\n",
        "\n",
        "        while line:\n",
        "            if line=='\\n':\n",
        "                if len(X[-1])>0:\n",
        "                    X.append([])\n",
        "                    Y.append([])\n",
        "            else:\n",
        "                word,ner=line.split()\n",
        "                assert ner in self.label_map\n",
        "                word=re.sub('\\d','0',word)      #replace all the digits with 0, this helps\n",
        "                X[-1].append(word)\n",
        "                Y[-1].append(self.label_map.index(ner))\n",
        "            line=f.readline()\n",
        "                \n",
        "        f.close()\n",
        "        if len(X[-1])==0:\n",
        "            X=X[:-1]\n",
        "            Y=Y[:-1]\n",
        "\n",
        "        self.label=Y\n",
        "        self.data_num=len(X)\n",
        "\n",
        "        #get word dictionary\n",
        "        if training:\n",
        "            dic_word=dictionary()\n",
        "            for sentence in X:\n",
        "                for word in sentence:\n",
        "                    dic_word.add_word(word)\n",
        "            dic_word.create_mapping()\n",
        "\n",
        "        #get word_ids: list of lists\n",
        "        #encode words str->id\n",
        "        self.word_ids=[]\n",
        "        for i in range(len(X)):\n",
        "            self.word_ids.append(list(map(lambda x:dic_word.get_id(x), X[i])))\n",
        "\n",
        "        #get character dictionary\n",
        "        if training:\n",
        "            dic_char=dictionary()\n",
        "            for sentence in X:\n",
        "                text=''.join(sentence)\n",
        "                for char in text:\n",
        "                    dic_char.add_word(char)\n",
        "            dic_char.create_mapping()\n",
        "\n",
        "        #get char_ids: list of lists of lists\n",
        "        self.char_ids=[]\n",
        "        for sentence in X:\n",
        "            s=[]\n",
        "            for word in sentence:\n",
        "                s.append(list(map(lambda x:dic_char.get_id(x), word)))\n",
        "            self.char_ids.append(s)\n",
        "\n",
        "        self.dic_word=dic_word\n",
        "        self.dic_char=dic_char\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.word_ids[index],self.char_ids[index],self.label[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_num\n",
        "     \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yY9jjDUBLfnj"
      },
      "outputs": [],
      "source": [
        "def expand_dic(dictionary,embedding_path,paths):\n",
        "            f=open(embedding_path,encoding=\"utf-8\")\n",
        "            line=f.readline()\n",
        "            word2emb={}\n",
        "            while line:\n",
        "                line=line.split()\n",
        "                word2emb[line[0]]=torch.from_numpy(np.array(line[1:],dtype=np.str).astype(np.float))\n",
        "                line=f.readline()\n",
        "\n",
        "            words=[]\n",
        "            for data_path in paths:\n",
        "                f=open(data_path,encoding='utf-8')\n",
        "                line=f.readline()\n",
        "                while line:\n",
        "                    if line=='\\n':\n",
        "                        pass\n",
        "                    else:\n",
        "                        word=line.split()[0]\n",
        "                        words.append(word)\n",
        "                    line=f.readline()\n",
        "                f.close()\n",
        "\n",
        "            train_len=dictionary.get_len()\n",
        "            for word in words:\n",
        "                if word not in dictionary.word2id and any([x in word2emb for x in [word,word.lower(),re.sub('\\d','0',word.lower())]]):\n",
        "                    dictionary.word2id[word]=dictionary.get_len()\n",
        "                    dictionary.id2word[dictionary.get_len()]=word\n",
        "                    dictionary.ordered_lis.append((word,0))\n",
        "            num_add=dictionary.get_len()-train_len\n",
        "            print(\"original word num: %d  expand num: %d\"%(train_len,num_add)) \n",
        "            return dictionary,word2emb\n",
        "\n",
        "def collate_batch(batch):\n",
        "            #input is a list of tuples\n",
        "            word_num=list(map(lambda x:len(x[0]),batch))\n",
        "            max_word_num=max(word_num)\n",
        "            word_ids=list(map(lambda x:x[0]+[0]*(max_word_num-len(x[0])),batch))\n",
        "            label_ids=list(map(lambda x:x[2]+[0]*(max_word_num-len(x[2])),batch))\n",
        "\n",
        "            max_word_length=max(list(map(lambda x:max([len(i) for i in x[1]]),batch)))\n",
        "            char_ids=[]\n",
        "            for tuple in batch:\n",
        "                s=[]\n",
        "                for word in tuple[1]:\n",
        "                    s.append(word+[0]*(max_word_length-len(word)))\n",
        "                s=s+[[0]*max_word_length for i in range((max_word_num-len(s)))]\n",
        "                char_ids.append(s)\n",
        "\n",
        "            word_num=torch.LongTensor(word_num)\n",
        "            word_ids=torch.LongTensor(word_ids)\n",
        "            char_ids=torch.LongTensor(char_ids)\n",
        "            label_ids=torch.LongTensor(label_ids)\n",
        "\n",
        "            return word_num,word_ids,char_ids,label_ids\n",
        "\n",
        "def log_sum_exp(matrix,dim):\n",
        "                maximum,_=matrix.max(dim=dim,keepdim=True)  #to avoid NaN\n",
        "                return (maximum+torch.log(torch.exp(matrix-maximum).sum(dim=dim,keepdim=True))).squeeze(1)\n",
        "\n",
        "\n",
        "def forward_alg(observation,transition,word_num):            \n",
        "            observation=observation.transpose(1,2)\n",
        "            transition=transition.unsqueeze(0).expand(observation.size(0),-1,-1)\n",
        "            alpha=torch.zeros_like(observation)\n",
        "            alpha[:,:,0:1]=observation[:,:,0:1]\n",
        "            for i in range(1,observation.size(2)):\n",
        "                alpha[:,:,i:i+1]=(observation[:,:,i]+log_sum_exp(alpha[:,:,i-1:i]+transition,dim=1)).unsqueeze(2)\n",
        "            end_label=alpha[:,10,1:]     #(batch_size, sequence_len)\n",
        "            return end_label.gather(1,word_num.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "def list_batch(pred,word_num,word_ids,label_ids,dic_word,label_map):\n",
        "    pred=pred.tolist()\n",
        "    word_num=word_num.tolist()\n",
        "    label_ids=label_ids.tolist()\n",
        "    word_ids=word_ids.tolist()\n",
        "\n",
        "    outputs=[]\n",
        "    for i in range(len(word_num)):\n",
        "        seq_len=word_num[i]\n",
        "        prediction=pred[i][:seq_len]\n",
        "        target=label_ids[i][:seq_len]\n",
        "        words=word_ids[i][:seq_len]\n",
        "        prediction=list(map(lambda x: label_map[x], prediction))\n",
        "        target=list(map(lambda x: label_map[x], target))\n",
        "        words=list(map(lambda x: dic_word.get_word(x), words))\n",
        "        for j in range(seq_len):\n",
        "            outputs.append(' '.join([words[j],target[j],prediction[j]]))\n",
        "        outputs.append('')\n",
        "    \n",
        "    return outputs  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2pxPf4PxaxaA"
      },
      "outputs": [],
      "source": [
        "class LSTM_CRF(nn.Module):\n",
        "    def __init__(self,word2emb,dic_word,dic_char):\n",
        "        super(LSTM_CRF, self).__init__()\n",
        "        word_emb_dim=300\n",
        "        word_lstm_dim=300\n",
        "        char_emb_dim=25\n",
        "        char_lstm_dim=25\n",
        "        label_num=9\n",
        "        dropout_rate=0.4\n",
        "            \n",
        "        word_emb=nn.Embedding(dic_word.get_len(),word_emb_dim,padding_idx=0)\n",
        "        for i in range(dic_word.get_len()):\n",
        "            word=dic_word.ordered_lis[i][0]\n",
        "            if word in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[word]\n",
        "            elif word.lower() in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[word.lower()]\n",
        "            elif re.sub('\\d','0',word.lower()) in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[re.sub('\\d','0',word.lower())]\n",
        "        #print(word_emb.weight.data[0])\n",
        "\n",
        "\n",
        "        char_emb=nn.Embedding(dic_char.get_len(),char_emb_dim,padding_idx=0)\n",
        "\n",
        "        self.char_emb=char_emb\n",
        "        self.char_lstm=nn.LSTM(char_emb_dim,char_lstm_dim,batch_first=True,bidirectional=True)\n",
        "        self.word_emb=word_emb\n",
        "        self.dropout=nn.Dropout(dropout_rate)\n",
        "        self.word_lstm1=nn.LSTM(word_emb_dim+char_lstm_dim*2, word_lstm_dim,batch_first=True,bidirectional=True)\n",
        "        self.word_lstm2=nn.LSTM(word_lstm_dim*2, word_lstm_dim,batch_first=True,bidirectional=True)\n",
        "        self.word_lstm3=nn.LSTM(word_lstm_dim*2, word_lstm_dim,batch_first=True,bidirectional=True)\n",
        "        self.word_lstm4=nn.LSTM(word_lstm_dim*2, word_lstm_dim,batch_first=True,bidirectional=True)\n",
        "        self.fc = nn.Linear(word_lstm_dim*2, label_num)\n",
        "        \n",
        "        #crf transition matrix parameter\n",
        "        self.transition=nn.Parameter(torch.full((label_num+2,label_num+2),math.log(1/label_num)))\n",
        "        \n",
        "        self.char_lstm_dim=char_lstm_dim\n",
        "        self.char_emb_dim=char_emb_dim\n",
        "        self.word_lstm_dim=word_lstm_dim\n",
        "        self.word_emb_dim=word_emb_dim\n",
        "        self.label_num=label_num\n",
        "        \n",
        "    def get_feature(self,word_num,word_ids,char_ids):\n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        char_input=self.char_emb(char_ids)\n",
        "        #print(char_input.size())    #4 dimensional\n",
        "        char_emb_dim=char_input.size(3)\n",
        "        word_len=char_input.size(2)\n",
        "        char_input=char_input.view(batch_size*sequence_len,word_len,char_emb_dim)\n",
        "        char_hidden,_=self.char_lstm(char_input)    #second output \"_\" is equal to char_output below\n",
        "        forward_=char_hidden[:,-1,:self.char_lstm_dim]\n",
        "        backward_=char_hidden[:,0,self.char_lstm_dim:]\n",
        "        char_output=torch.cat((forward_,backward_),dim=-1)\n",
        "        char_output=char_output.view(batch_size,sequence_len,self.char_lstm_dim*2)\n",
        "\n",
        "        index=torch.LongTensor(list(range(sequence_len))).cuda().unsqueeze(0).expand(batch_size,sequence_len)\n",
        "        condition=word_num.unsqueeze(1).expand(batch_size,sequence_len)>index\n",
        "        mask=torch.where(condition,torch.ones(1,).cuda(),torch.zeros(1,).cuda()).unsqueeze(2)\n",
        "        char_output*=mask   #to mask all the padding tokens\n",
        "\n",
        "        word_feature=self.word_emb(word_ids)\n",
        "        word_feature=torch.cat((word_feature,char_output),dim=-1)\n",
        "        word_feature=self.dropout(word_feature)\n",
        "        h1,(h1_T,c1_T)=self.word_lstm1(word_feature)\n",
        "        h2,(h2_T,c2_T)=self.word_lstm2(h1)\n",
        "        h3,(h3_T,c3_T)=self.word_lstm3(h2)\n",
        "        h4,(h4_T,c4_T)=self.word_lstm4(h3)        \n",
        "        word_feature = self.fc(h4)\n",
        "        \n",
        "        word_feature*=mask\n",
        "      \n",
        "        return word_feature,mask\n",
        "    \n",
        "    def forward(self,word_num,word_ids,char_ids,label_ids):    \n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        word_feature,mask=self.get_feature(word_num,word_ids,char_ids)\n",
        "        \n",
        "        #compute numerator: the score of target label sequence\n",
        "        numerator=word_feature.gather(2,label_ids.unsqueeze(2)).squeeze(2).sum(dim=1)\n",
        "        #print(numerator.size())\n",
        "        padded_label=torch.cat((torch.full((batch_size,1),9,dtype=torch.long).cuda(), label_ids), dim=1)\n",
        "\n",
        "        #print(self.transition[(padded_label[:,:-1],padded_label[:,1:])].size()) \n",
        "        #a tensor can be indexed by several LongTensors or lists, each of them corresponds with an axis\n",
        "        trans_score=self.transition[(padded_label[:,:-1],padded_label[:,1:])]   #size(batch_size,sequence_len)\n",
        "        trans_score*=mask.squeeze(2)\n",
        "        numerator+=trans_score.sum(dim=1)\n",
        "        last_label=(padded_label.gather(1,word_num.unsqueeze(1))).squeeze()\n",
        "        numerator+=self.transition[(last_label,torch.full((batch_size,),10,dtype=torch.long).cuda())]\n",
        "        \n",
        "        #prepare the observation matrix\n",
        "        small=-1000\n",
        "        se_label=torch.full((batch_size,sequence_len,2),small,dtype=torch.float).cuda()*mask\n",
        "        observation=torch.cat((word_feature,se_label),dim=2)\n",
        "        observation=torch.cat((torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda(),\n",
        "                                observation,\n",
        "                                torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda()),dim=1)\n",
        "        \n",
        "       \n",
        "        observation[:,0,9]=0\n",
        "        observation[:,-1,10]=0\n",
        "\n",
        "        denominator=forward_alg(observation,self.transition,word_num)   #the score of all the label sequences\n",
        "        loss=-(numerator-denominator)\n",
        "\n",
        "        return torch.mean(loss)\n",
        "    \n",
        "    #viterbi decoder\n",
        "    def decode(self,word_num,word_ids,char_ids):\n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        word_feature,mask=self.get_feature(word_num,word_ids,char_ids)\n",
        "\n",
        "        index=torch.LongTensor(list(range(sequence_len))).cuda().unsqueeze(0).expand(batch_size,sequence_len)\n",
        "        condition=word_num.unsqueeze(1).expand(batch_size,sequence_len)==index\n",
        "        end_mask=torch.where(condition,torch.ones(1,).cuda(),torch.zeros(1,).cuda()).unsqueeze(2)\n",
        "\n",
        "        small=-1000\n",
        "        constrain=torch.full((batch_size,sequence_len,self.label_num+2),small,dtype=torch.float).cuda()*end_mask\n",
        "        constrain[:,:,10]=0         #tensor \"constrain\" is used to make sure all the paths finish at [END] state\n",
        "\n",
        "        se_label=torch.full((batch_size,sequence_len,2),small,dtype=torch.float).cuda()*mask    #correspond with Start and End label\n",
        "        observation=torch.cat((word_feature,se_label),dim=2)\n",
        "        observation+=constrain\n",
        "        observation=torch.cat((torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda(),\n",
        "                                observation,\n",
        "                                torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda()),dim=1)\n",
        "        observation[:,0,9]=0\n",
        "        observation[:,-1,10]=0\n",
        "       \n",
        "        observation=observation.transpose(1,2)\n",
        "        path=torch.zeros_like(observation).long().cuda()\n",
        "        transition=self.transition.unsqueeze(0).expand(batch_size,-1,-1)\n",
        "        z=observation[:,:,0:1]\n",
        "        for i in range(1,observation.size(2)):\n",
        "            values,indices=(z+transition).max(dim=1)\n",
        "            path[:,:,i]=indices\n",
        "            values+=observation[:,:,i]\n",
        "            z=values.unsqueeze(2)\n",
        "\n",
        "        last=path[:,10,-1:]\n",
        "        pred=last\n",
        "        for i in range(path.size(2)-2,1,-1):\n",
        "            last=path[:,:,i].gather(1,last)\n",
        "            pred=torch.cat((last,pred),dim=1)   \n",
        "        \n",
        "       \n",
        "        #validation     this step is unnecessary, just to make sure there is nothing wrong\n",
        "        pred_=torch.cat((pred,torch.full((batch_size,1),10,dtype=torch.long).cuda()),dim=1)\n",
        "        condition=pred_.gather(1,word_num.unsqueeze(1)).squeeze(1)==10\n",
        "        assert condition.size(0)==(condition.sum().item())\n",
        "        #print(\"validation passed\")\n",
        "\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GljQpo2xMIMt",
        "outputId": "e632e762-8fca-40a3-c6a6-aad491920d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conlleval\n",
            "  Downloading conlleval-0.2-py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: conlleval\n",
            "Successfully installed conlleval-0.2\n"
          ]
        }
      ],
      "source": [
        "pip install conlleval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNTctkV9UudH",
        "outputId": "273946fc-2ce6-4501-8707-18878e3eedf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original word num: 7687  expand num: 1776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:14<00:00, 12.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:  mean loss: 6.0512  f1 score: 52.72  best: 52.72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:14<00:00, 12.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1:  mean loss: 2.6521  f1 score: 69.94  best: 69.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 12.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2:  mean loss: 1.7879  f1 score: 73.28  best: 73.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 12.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3:  mean loss: 1.3297  f1 score: 75.37  best: 75.37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 12.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4:  mean loss: 1.0093  f1 score: 76.34  best: 76.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 12.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5:  mean loss: 0.7970  f1 score: 76.76  best: 76.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6:  mean loss: 0.6413  f1 score: 77.03  best: 77.03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7:  mean loss: 0.5549  f1 score: 78.75  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8:  mean loss: 0.4590  f1 score: 78.71  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9:  mean loss: 0.3515  f1 score: 78.25  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 12.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10:  mean loss: 0.3063  f1 score: 78.25  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:16<00:00, 11.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 11:  mean loss: 0.2738  f1 score: 78.57  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 12:  mean loss: 0.2271  f1 score: 78.71  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 13:  mean loss: 0.1953  f1 score: 78.35  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 14:  mean loss: 0.1922  f1 score: 78.74  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 15:  mean loss: 0.1669  f1 score: 78.75  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16:  mean loss: 0.1435  f1 score: 78.75  best: 78.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 17:  mean loss: 0.1200  f1 score: 79.98  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 18:  mean loss: 0.1249  f1 score: 76.37  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 19:  mean loss: 0.1244  f1 score: 78.20  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 20:  mean loss: 0.1344  f1 score: 78.26  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 21:  mean loss: 0.1055  f1 score: 79.52  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 22:  mean loss: 0.1122  f1 score: 78.43  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 23:  mean loss: 0.0845  f1 score: 78.62  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 24:  mean loss: 0.0828  f1 score: 78.93  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 25:  mean loss: 0.0845  f1 score: 78.83  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:16<00:00, 11.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 26:  mean loss: 0.0835  f1 score: 78.70  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 27:  mean loss: 0.0875  f1 score: 78.62  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 28:  mean loss: 0.0886  f1 score: 79.37  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 29:  mean loss: 0.0901  f1 score: 79.30  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 30:  mean loss: 0.0745  f1 score: 79.60  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 31:  mean loss: 0.0567  f1 score: 78.38  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 32:  mean loss: 0.0709  f1 score: 79.20  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 33:  mean loss: 0.0622  f1 score: 78.45  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 34:  mean loss: 0.0617  f1 score: 78.80  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 35:  mean loss: 0.0681  f1 score: 78.76  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 36:  mean loss: 0.0550  f1 score: 79.44  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 37:  mean loss: 0.0605  f1 score: 79.14  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 38:  mean loss: 0.0490  f1 score: 77.36  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 39:  mean loss: 0.0508  f1 score: 77.79  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 40:  mean loss: 0.0569  f1 score: 78.39  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 41:  mean loss: 0.0543  f1 score: 78.56  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 42:  mean loss: 0.0495  f1 score: 78.42  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 43:  mean loss: 0.0527  f1 score: 78.05  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 44:  mean loss: 0.0445  f1 score: 78.83  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 45:  mean loss: 0.0485  f1 score: 78.86  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 46:  mean loss: 0.0465  f1 score: 79.15  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 47:  mean loss: 0.0496  f1 score: 79.21  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 48:  mean loss: 0.0405  f1 score: 79.29  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 49:  mean loss: 0.0339  f1 score: 79.52  best: 79.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 50:  mean loss: 0.0425  f1 score: 80.01  best: 80.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 51:  mean loss: 0.0459  f1 score: 80.06  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 52:  mean loss: 0.0419  f1 score: 78.68  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 53:  mean loss: 0.0365  f1 score: 79.08  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 54:  mean loss: 0.0429  f1 score: 79.09  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 55:  mean loss: 0.0432  f1 score: 78.74  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 56:  mean loss: 0.0423  f1 score: 79.32  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 57:  mean loss: 0.0380  f1 score: 78.60  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 58:  mean loss: 0.0306  f1 score: 79.02  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 59:  mean loss: 0.0323  f1 score: 79.09  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 60:  mean loss: 0.0351  f1 score: 78.72  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 61:  mean loss: 0.0308  f1 score: 79.43  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 62:  mean loss: 0.0333  f1 score: 79.78  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 63:  mean loss: 0.0420  f1 score: 79.15  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 64:  mean loss: 0.0372  f1 score: 79.00  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 65:  mean loss: 0.0355  f1 score: 79.93  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 66:  mean loss: 0.0287  f1 score: 78.98  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 67:  mean loss: 0.0264  f1 score: 79.18  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 68:  mean loss: 0.0294  f1 score: 79.10  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 69:  mean loss: 0.0262  f1 score: 79.75  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 70:  mean loss: 0.0311  f1 score: 79.40  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 71:  mean loss: 0.0270  f1 score: 79.61  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 72:  mean loss: 0.0283  f1 score: 78.94  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 73:  mean loss: 0.0277  f1 score: 79.75  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 74:  mean loss: 0.0302  f1 score: 79.27  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 12.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 75:  mean loss: 0.0295  f1 score: 78.75  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 76:  mean loss: 0.0299  f1 score: 79.17  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 77:  mean loss: 0.0266  f1 score: 79.30  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 78:  mean loss: 0.0269  f1 score: 79.34  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 79:  mean loss: 0.0255  f1 score: 79.24  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 80:  mean loss: 0.0251  f1 score: 79.38  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 81:  mean loss: 0.0260  f1 score: 79.44  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 82:  mean loss: 0.0235  f1 score: 79.39  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 83:  mean loss: 0.0301  f1 score: 79.91  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 84:  mean loss: 0.0247  f1 score: 79.85  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 85:  mean loss: 0.0226  f1 score: 79.46  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 12.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 86:  mean loss: 0.0249  f1 score: 79.44  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 87:  mean loss: 0.0219  f1 score: 79.44  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 88:  mean loss: 0.0253  f1 score: 79.42  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 89:  mean loss: 0.0280  f1 score: 79.85  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 90:  mean loss: 0.0244  f1 score: 79.49  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 91:  mean loss: 0.0244  f1 score: 79.53  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 11.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 92:  mean loss: 0.0196  f1 score: 79.46  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:15<00:00, 12.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 93:  mean loss: 0.0191  f1 score: 79.45  best: 80.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 15/183 [00:01<00:13, 12.60it/s]"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def my_plot(epochs, datalist):\n",
        "    plt.plot(epochs, datalist)\n",
        "    # Add title and axis names\n",
        "    plt.title('Mean Loss by Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Mean Loss') \n",
        "    #legend = plt.legend(loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE=64\n",
        "LR=0.001\n",
        "CLIP=5.\n",
        "num_epochs = 200\n",
        "train_dataset=I2B2DatasetReader('./datafiles/train.txt',dictionary(),dictionary(),training=True)\n",
        "train_dataset.dic_word,word2emb=expand_dic(train_dataset.dic_word,\"datafiles/glove.6B.300d.txt\",['./datafiles/dev.txt','./datafiles/test.txt'])\n",
        "dev_dataset=I2B2DatasetReader('./datafiles/dev.txt',train_dataset.dic_word,train_dataset.dic_char)\n",
        "test_dataset=I2B2DatasetReader('./datafiles/test.txt',train_dataset.dic_word,train_dataset.dic_char)\n",
        "\n",
        "train_loader=DataLoader(train_dataset,BATCH_SIZE,shuffle=True,num_workers=2,collate_fn=collate_batch)\n",
        "dev_loader=DataLoader(dev_dataset,BATCH_SIZE,shuffle=False,num_workers=2,collate_fn=collate_batch)\n",
        "test_loader=DataLoader(test_dataset,BATCH_SIZE,shuffle=False,num_workers=2,collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "model = LSTM_CRF(word2emb,train_dataset.dic_word,train_dataset.dic_char).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),LR)\n",
        "precision_list = []\n",
        "accuracy_list = []\n",
        "recall_list = []\n",
        "best_score=0\n",
        "best_f1score_validation = 0\n",
        "loss_vals=  []\n",
        "loss_vals_validation =  []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss=[]\n",
        "    pbar=tqdm(total=len(train_loader))\n",
        "    for i,(word_num,word_ids,char_ids,label_ids) in enumerate(train_loader):\n",
        "        loss=model(word_num.cuda(),word_ids.cuda(),char_ids.cuda(),label_ids.cuda())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #nn.utils.clip_grad_norm_(lstm_crf.parameters(),CLIP)\n",
        "        optimizer.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "    loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
        "    mean_loss=torch.mean(torch.tensor(epoch_loss)).item()\n",
        "   \n",
        "\n",
        "    model.eval()\n",
        "    f1_score=0\n",
        "\n",
        "    \n",
        "    #for loader in (test_loader):\n",
        "    testloader=test_loader\n",
        "\n",
        "    outputs=[]\n",
        "    for i,(word_num,word_ids,char_ids,label_ids) in enumerate(testloader):\n",
        "        word_num,word_ids,char_ids,label_ids=word_num.cuda(),word_ids.cuda(),char_ids.cuda(),label_ids.cuda()\n",
        "        pred=model.decode(word_num,word_ids,char_ids)\n",
        "        outputs+=list_batch(pred,word_num,word_ids,label_ids, train_dataset.dic_word, train_dataset.label_map)\n",
        "\n",
        "    f=open('outputs.txt','w',encoding='utf-8')\n",
        "    f.write('\\n'.join(outputs))\n",
        "    f.close()\n",
        "    \n",
        "    os.system(\"python -m conlleval outputs.txt > results\")\n",
        "    #os.system(\"./conlleval < outputs.txt > results\")\n",
        "    f=open('results','r',encoding='utf-8')\n",
        "    \n",
        "    elems = re.split('[:;]+', f.readlines()[1])\n",
        "    f1_score=float(elems[7])\n",
        "    # recall_list.append(elems[5])\n",
        "    # precision_list.append(elems[3])\n",
        "    # accuracy_list.append(elems[1])\n",
        "    f.close()\n",
        "    best_score=max(best_score,f1_score)\n",
        "    #print('epoch %d:  mean accuracy: %.4f  mean precision: %.2f  mean recall: %.2f'%(epoch,mean_accuracy,mean_precision,mean_recall))\n",
        "\n",
        "    print('epoch %d:  mean loss: %.4f  f1 score: %.2f  best: %.2f'%(epoch,mean_loss,f1_score,best_score))\n",
        "\n",
        "my_plot(np.linspace(1, num_epochs, num_epochs).astype(int), loss_vals)  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BIDIR-LSTM-CRF-FC.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP49Q1NwWtAyoNYr7Q9cNAx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}