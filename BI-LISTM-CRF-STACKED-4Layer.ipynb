{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mandar33/BI-LSTM-CRF-FC/blob/main/BI-LISTM-CRF-STACKED-4Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQoZMdHRfW_x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import io\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_SZR0X6LUoV"
      },
      "outputs": [],
      "source": [
        "class dictionary():\n",
        "    def __init__(self):\n",
        "        self.word_freq={}\n",
        "        self.id2word={}\n",
        "        self.word2id={}\n",
        "    \n",
        "    def add_word(self,word):\n",
        "        if word in self.word_freq:\n",
        "            self.word_freq[word]+=1\n",
        "        else:\n",
        "            self.word_freq[word]=1\n",
        "        \n",
        "    def create_mapping(self):\n",
        "        self.word_freq['[PAD]']=1000001\n",
        "        self.word_freq['[UNK]']=1000000\n",
        "        c_unk=0\n",
        "        dic_items=[]\n",
        "        for k in self.word_freq.keys():\n",
        "            if self.word_freq[k]>1 or np.random.uniform()>0.5:\n",
        "                dic_items.append((k,self.word_freq[k]))\n",
        "            else:\n",
        "                c_unk+=1\n",
        "        ordered_lis=sorted( dic_items, key=lambda x: (-x[1],x[0]))\n",
        "        assert ordered_lis[0][0]=='[PAD]'\n",
        "        self.id2word=dict([(i,ordered_lis[i][0]) for i in range(len(ordered_lis))])\n",
        "        self.word2id=dict([(ordered_lis[i][0],i) for i in range(len(ordered_lis))])\n",
        "        self.ordered_lis=ordered_lis\n",
        "        return c_unk\n",
        "\n",
        "    def get_id(self,word):\n",
        "        if word in self.word2id:\n",
        "            return self.word2id[word]\n",
        "        else:\n",
        "            return 1\n",
        "    \n",
        "    def get_word(self,idx):\n",
        "        return self.id2word[idx]\n",
        "    \n",
        "    def get_len(self):\n",
        "        return len(self.id2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbxxflzpUrhn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmEiiZzvLZ3r"
      },
      "outputs": [],
      "source": [
        "class I2B2DatasetReader(Dataset):\n",
        "    def __init__(self,data_path,dic_word,dic_char,training=False):\n",
        "        super(I2B2DatasetReader,self).__init__()\n",
        "        f=open(data_path,encoding='utf-8')\n",
        "        X=[[]]\n",
        "        Y=[[]]\n",
        "\n",
        "        line=f.readline()\n",
        "        self.label_map=[\"O\",\"B-problem\",\"I-problem\",\"B-test\",\"I-test\",\"B-treatment\",\"I-treatment\",\"B-MISC\",\"I-MISC\"]\n",
        "        self.label_num=len(self.label_map)\n",
        "\n",
        "        while line:\n",
        "            if line=='\\n':\n",
        "                if len(X[-1])>0:\n",
        "                    X.append([])\n",
        "                    Y.append([])\n",
        "            else:\n",
        "                word,ner=line.split()\n",
        "                assert ner in self.label_map\n",
        "                word=re.sub('\\d','0',word)      #replace all the digits with 0, this helps\n",
        "                X[-1].append(word)\n",
        "                Y[-1].append(self.label_map.index(ner))\n",
        "            line=f.readline()\n",
        "                \n",
        "        f.close()\n",
        "        if len(X[-1])==0:\n",
        "            X=X[:-1]\n",
        "            Y=Y[:-1]\n",
        "\n",
        "        self.label=Y\n",
        "        self.data_num=len(X)\n",
        "\n",
        "        #get word dictionary\n",
        "        if training:\n",
        "            dic_word=dictionary()\n",
        "            for sentence in X:\n",
        "                for word in sentence:\n",
        "                    dic_word.add_word(word)\n",
        "            dic_word.create_mapping()\n",
        "\n",
        "        #get word_ids: list of lists\n",
        "        #encode words str->id\n",
        "        self.word_ids=[]\n",
        "        for i in range(len(X)):\n",
        "            self.word_ids.append(list(map(lambda x:dic_word.get_id(x), X[i])))\n",
        "\n",
        "        #get character dictionary\n",
        "        if training:\n",
        "            dic_char=dictionary()\n",
        "            for sentence in X:\n",
        "                text=''.join(sentence)\n",
        "                for char in text:\n",
        "                    dic_char.add_word(char)\n",
        "            dic_char.create_mapping()\n",
        "\n",
        "        #get char_ids: list of lists of lists\n",
        "        self.char_ids=[]\n",
        "        for sentence in X:\n",
        "            s=[]\n",
        "            for word in sentence:\n",
        "                s.append(list(map(lambda x:dic_char.get_id(x), word)))\n",
        "            self.char_ids.append(s)\n",
        "\n",
        "        self.dic_word=dic_word\n",
        "        self.dic_char=dic_char\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.word_ids[index],self.char_ids[index],self.label[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_num\n",
        "     \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY9jjDUBLfnj"
      },
      "outputs": [],
      "source": [
        "def expand_dic(dictionary,embedding_path,paths):\n",
        "            f=open(embedding_path,encoding=\"utf-8\")\n",
        "            line=f.readline()\n",
        "            word2emb={}\n",
        "            while line:\n",
        "                line=line.split()\n",
        "                word2emb[line[0]]=torch.from_numpy(np.array(line[1:],dtype=np.str).astype(np.float))\n",
        "                line=f.readline()\n",
        "\n",
        "            words=[]\n",
        "            for data_path in paths:\n",
        "                f=open(data_path,encoding='utf-8')\n",
        "                line=f.readline()\n",
        "                while line:\n",
        "                    if line=='\\n':\n",
        "                        pass\n",
        "                    else:\n",
        "                        word=line.split()[0]\n",
        "                        words.append(word)\n",
        "                    line=f.readline()\n",
        "                f.close()\n",
        "\n",
        "            train_len=dictionary.get_len()\n",
        "            for word in words:\n",
        "                if word not in dictionary.word2id and any([x in word2emb for x in [word,word.lower(),re.sub('\\d','0',word.lower())]]):\n",
        "                    dictionary.word2id[word]=dictionary.get_len()\n",
        "                    dictionary.id2word[dictionary.get_len()]=word\n",
        "                    dictionary.ordered_lis.append((word,0))\n",
        "            num_add=dictionary.get_len()-train_len\n",
        "            print(\"original word num: %d  expand num: %d\"%(train_len,num_add)) \n",
        "            return dictionary,word2emb\n",
        "\n",
        "def collate_batch(batch):\n",
        "            #input is a list of tuples\n",
        "            word_num=list(map(lambda x:len(x[0]),batch))\n",
        "            max_word_num=max(word_num)\n",
        "            word_ids=list(map(lambda x:x[0]+[0]*(max_word_num-len(x[0])),batch))\n",
        "            label_ids=list(map(lambda x:x[2]+[0]*(max_word_num-len(x[2])),batch))\n",
        "\n",
        "            max_word_length=max(list(map(lambda x:max([len(i) for i in x[1]]),batch)))\n",
        "            char_ids=[]\n",
        "            for tuple in batch:\n",
        "                s=[]\n",
        "                for word in tuple[1]:\n",
        "                    s.append(word+[0]*(max_word_length-len(word)))\n",
        "                s=s+[[0]*max_word_length for i in range((max_word_num-len(s)))]\n",
        "                char_ids.append(s)\n",
        "\n",
        "            word_num=torch.LongTensor(word_num)\n",
        "            word_ids=torch.LongTensor(word_ids)\n",
        "            char_ids=torch.LongTensor(char_ids)\n",
        "            label_ids=torch.LongTensor(label_ids)\n",
        "\n",
        "            return word_num,word_ids,char_ids,label_ids\n",
        "\n",
        "def forward_alg(observation,transition,word_num):\n",
        "\n",
        "            def log_sum_exp(matrix,dim):\n",
        "                maximum,_=matrix.max(dim=dim,keepdim=True)  #to avoid NaN\n",
        "                return (maximum+torch.log(torch.exp(matrix-maximum).sum(dim=dim,keepdim=True))).squeeze(1)\n",
        "\n",
        "            observation=observation.transpose(1,2)\n",
        "            transition=transition.unsqueeze(0).expand(observation.size(0),-1,-1)\n",
        "            alpha=torch.zeros_like(observation)\n",
        "            alpha[:,:,0:1]=observation[:,:,0:1]\n",
        "            for i in range(1,observation.size(2)):\n",
        "                alpha[:,:,i:i+1]=(observation[:,:,i]+log_sum_exp(alpha[:,:,i-1:i]+transition,dim=1)).unsqueeze(2)\n",
        "            end_label=alpha[:,10,1:]     #(batch_size, sequence_len)\n",
        "            return end_label.gather(1,word_num.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "def list_batch(pred,word_num,word_ids,label_ids,dic_word,label_map):\n",
        "    pred=pred.tolist()\n",
        "    word_num=word_num.tolist()\n",
        "    label_ids=label_ids.tolist()\n",
        "    word_ids=word_ids.tolist()\n",
        "\n",
        "    outputs=[]\n",
        "    for i in range(len(word_num)):\n",
        "        seq_len=word_num[i]\n",
        "        prediction=pred[i][:seq_len]\n",
        "        target=label_ids[i][:seq_len]\n",
        "        words=word_ids[i][:seq_len]\n",
        "        prediction=list(map(lambda x: label_map[x], prediction))\n",
        "        target=list(map(lambda x: label_map[x], target))\n",
        "        words=list(map(lambda x: dic_word.get_word(x), words))\n",
        "        for j in range(seq_len):\n",
        "            outputs.append(' '.join([words[j],target[j],prediction[j]]))\n",
        "        outputs.append('')\n",
        "    \n",
        "    return outputs  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2pxPf4PxaxaA"
      },
      "outputs": [],
      "source": [
        "class LSTM_CRF(nn.Module):\n",
        "    def __init__(self,word2emb,dic_word,dic_char):\n",
        "        super(LSTM_CRF, self).__init__()\n",
        "        word_emb_dim=300\n",
        "        word_lstm_dim=300\n",
        "        char_emb_dim=25\n",
        "        char_lstm_dim=25\n",
        "        label_num=9\n",
        "        dropout_rate=0.4\n",
        "            \n",
        "        word_emb=nn.Embedding(dic_word.get_len(),word_emb_dim,padding_idx=0)\n",
        "        for i in range(dic_word.get_len()):\n",
        "            word=dic_word.ordered_lis[i][0]\n",
        "            if word in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[word]\n",
        "            elif word.lower() in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[word.lower()]\n",
        "            elif re.sub('\\d','0',word.lower()) in word2emb:\n",
        "                word_emb.weight.data[i]=word2emb[re.sub('\\d','0',word.lower())]\n",
        "        #print(word_emb.weight.data[0])\n",
        "\n",
        "\n",
        "        char_emb=nn.Embedding(dic_char.get_len(),char_emb_dim,padding_idx=0)\n",
        "\n",
        "        self.char_emb=char_emb\n",
        "        self.char_lstm=nn.LSTM(char_emb_dim,char_lstm_dim,num_layers = 4,batch_first=True,bidirectional=True)\n",
        "        self.word_emb=word_emb\n",
        "        self.dropout=nn.Dropout(dropout_rate)\n",
        "        self.word_lstm=nn.LSTM(word_emb_dim+char_lstm_dim*2, word_lstm_dim,batch_first=True,num_layers = 4,bidirectional=True)      \n",
        "        self.fc=  nn.Linear((word_emb_dim)*2,label_num)\n",
        "\n",
        "        #crf\n",
        "        self.transition=nn.Parameter(torch.full((label_num+2,label_num+2),math.log(1/label_num)))\n",
        "        \n",
        "        self.char_lstm_dim=char_lstm_dim\n",
        "        self.char_emb_dim=char_emb_dim\n",
        "        self.word_lstm_dim=word_lstm_dim\n",
        "        self.word_emb_dim=word_emb_dim\n",
        "        self.label_num=label_num\n",
        "        \n",
        "    def get_feature(self,word_num,word_ids,char_ids):\n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        char_input=self.char_emb(char_ids)\n",
        "        #print(char_input.size())    #4 dimensional\n",
        "        char_emb_dim=char_input.size(3)\n",
        "        word_len=char_input.size(2)\n",
        "        char_input=char_input.view(batch_size*sequence_len,word_len,char_emb_dim)\n",
        "        char_hidden,_=self.char_lstm(char_input)    #second output \"_\" is equal to char_output below\n",
        "        forward_=char_hidden[:,-1,:self.char_lstm_dim]\n",
        "        backward_=char_hidden[:,0,self.char_lstm_dim:]\n",
        "        char_output=torch.cat((forward_,backward_),dim=-1)\n",
        "        char_output=char_output.view(batch_size,sequence_len,self.char_lstm_dim*2)\n",
        "\n",
        "        index=torch.LongTensor(list(range(sequence_len))).cuda().unsqueeze(0).expand(batch_size,sequence_len)\n",
        "        condition=word_num.unsqueeze(1).expand(batch_size,sequence_len)>index\n",
        "        mask=torch.where(condition,torch.ones(1,).cuda(),torch.zeros(1,).cuda()).unsqueeze(2)\n",
        "        char_output*=mask   #to mask all the padding tokens\n",
        "\n",
        "        word_feature=self.word_emb(word_ids)\n",
        "        word_feature=torch.cat((word_feature,char_output),dim=-1)\n",
        "        word_feature=self.dropout(word_feature)\n",
        "        word_feature,_=self.word_lstm(word_feature)\n",
        "\n",
        "        #word_feature=self.hidden(word_feature)\n",
        "        word_feature = self.fc(word_feature)\n",
        "        \n",
        "        word_feature*=mask\n",
        "      \n",
        "        return word_feature,mask\n",
        "    \n",
        "    def forward(self,word_num,word_ids,char_ids,label_ids):    \n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        word_feature,mask=self.get_feature(word_num,word_ids,char_ids)\n",
        "        \n",
        "        #compute numerator: the score of target label sequence\n",
        "        numerator=word_feature.gather(2,label_ids.unsqueeze(2)).squeeze(2).sum(dim=1)\n",
        "        #print(numerator.size())\n",
        "        padded_label=torch.cat((torch.full((batch_size,1),9,dtype=torch.long).cuda(), label_ids), dim=1)\n",
        "\n",
        "        #print(self.transition[(padded_label[:,:-1],padded_label[:,1:])].size()) \n",
        "        #a tensor can be indexed by several LongTensors or lists, each of them corresponds with an axis\n",
        "        trans_score=self.transition[(padded_label[:,:-1],padded_label[:,1:])]   #size(batch_size,sequence_len)\n",
        "        trans_score*=mask.squeeze(2)\n",
        "        numerator+=trans_score.sum(dim=1)\n",
        "        last_label=(padded_label.gather(1,word_num.unsqueeze(1))).squeeze()\n",
        "        numerator+=self.transition[(last_label,torch.full((batch_size,),10,dtype=torch.long).cuda())]\n",
        "        \n",
        "        #prepare observation matrix\n",
        "        small=-1000\n",
        "        se_label=torch.full((batch_size,sequence_len,2),small,dtype=torch.float).cuda()*mask\n",
        "        observation=torch.cat((word_feature,se_label),dim=2)\n",
        "        observation=torch.cat((torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda(),\n",
        "                                observation,\n",
        "                                torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda()),dim=1)\n",
        "        \n",
        "       \n",
        "      \n",
        "\n",
        "        observation[:,0,9]=0\n",
        "        observation[:,-1,10]=0\n",
        "\n",
        "        denominator=forward_alg(observation,self.transition,word_num)   #the score of all the label sequences\n",
        "        loss=-(numerator-denominator)\n",
        "\n",
        "        return torch.mean(loss)\n",
        "    \n",
        "    def decode(self,word_num,word_ids,char_ids):\n",
        "        batch_size=word_ids.size(0)\n",
        "        sequence_len=word_ids.size(1)\n",
        "        word_feature,mask=self.get_feature(word_num,word_ids,char_ids)\n",
        "\n",
        "        index=torch.LongTensor(list(range(sequence_len))).cuda().unsqueeze(0).expand(batch_size,sequence_len)\n",
        "        condition=word_num.unsqueeze(1).expand(batch_size,sequence_len)==index\n",
        "        end_mask=torch.where(condition,torch.ones(1,).cuda(),torch.zeros(1,).cuda()).unsqueeze(2)\n",
        "\n",
        "        small=-1000\n",
        "        constrain=torch.full((batch_size,sequence_len,self.label_num+2),small,dtype=torch.float).cuda()*end_mask\n",
        "        constrain[:,:,10]=0         #tensor \"constrain\" is used to make sure all the paths finish at [END] state\n",
        "\n",
        "        se_label=torch.full((batch_size,sequence_len,2),small,dtype=torch.float).cuda()*mask    #correspond with Start and End label\n",
        "        observation=torch.cat((word_feature,se_label),dim=2)\n",
        "        observation+=constrain\n",
        "        observation=torch.cat((torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda(),\n",
        "                                observation,\n",
        "                                torch.full((batch_size,1,self.label_num+2),small,dtype=torch.float).cuda()),dim=1)\n",
        "        observation[:,0,9]=0\n",
        "        observation[:,-1,10]=0\n",
        "        \n",
        "        #viterbi\n",
        "        observation=observation.transpose(1,2)\n",
        "        path=torch.zeros_like(observation).long().cuda()\n",
        "        transition=self.transition.unsqueeze(0).expand(batch_size,-1,-1)\n",
        "        z=observation[:,:,0:1]\n",
        "        for i in range(1,observation.size(2)):\n",
        "            values,indices=(z+transition).max(dim=1)\n",
        "            path[:,:,i]=indices\n",
        "            values+=observation[:,:,i]\n",
        "            z=values.unsqueeze(2)\n",
        "\n",
        "        last=path[:,10,-1:]\n",
        "        pred=last\n",
        "        for i in range(path.size(2)-2,1,-1):\n",
        "            last=path[:,:,i].gather(1,last)\n",
        "            pred=torch.cat((last,pred),dim=1)   \n",
        "        \n",
        "        #pred size: batch_size,sequence_len\n",
        "        #print(pred.size())\n",
        "\n",
        "        #validation     this step is unnecessary, just make sure there is nothing wrong\n",
        "        pred_=torch.cat((pred,torch.full((batch_size,1),10,dtype=torch.long).cuda()),dim=1)\n",
        "        condition=pred_.gather(1,word_num.unsqueeze(1)).squeeze(1)==10\n",
        "        assert condition.size(0)==(condition.sum().item())\n",
        "        #print(\"validation passed\")\n",
        "\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GljQpo2xMIMt",
        "outputId": "6c0dc7fa-4228-4eb8-e529-39dd812eb55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conlleval in /usr/local/lib/python3.7/dist-packages (0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install conlleval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def my_plot(epochs, datalist):\n",
        "    plt.plot(epochs, datalist)\n",
        "    # Add title and axis names\n",
        "    plt.title('Mean Loss by Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Mean Loss') \n",
        "    #legend = plt.legend(loc='upper right')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OoVsC7DSsggt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNTctkV9UudH",
        "outputId": "55b408f9-8783-4264-ebbf-b83fa75ed613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original word num: 7722  expand num: 1531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:17<00:00, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:  mean loss: 6.2020  f1 score: 48.56  best: 48.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:17<00:00, 10.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1:  mean loss: 2.9107  f1 score: 66.94  best: 66.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 183/183 [00:17<00:00, 10.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2:  mean loss: 1.9060  f1 score: 72.03  best: 72.03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 33/183 [00:03<00:12, 11.62it/s]"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE=64 \n",
        "LR=0.001\n",
        "CLIP=5.\n",
        "num_epochs = 200\n",
        "train_dataset=I2B2DatasetReader('./datafiles/train.txt',dictionary(),dictionary(),training=True)\n",
        "train_dataset.dic_word,word2emb=augment_vocab(train_dataset.dic_word,\"datafiles/glove.6B.300d.txt\",['./datafiles/dev.txt','./datafiles/test.txt'])\n",
        "dev_dataset=I2B2DatasetReader('./datafiles/dev.txt',train_dataset.dic_word,train_dataset.dic_char)\n",
        "test_dataset=I2B2DatasetReader('./datafiles/test.txt',train_dataset.dic_word,train_dataset.dic_char)\n",
        "\n",
        "train_loader=DataLoader(train_dataset,BATCH_SIZE,shuffle=True,num_workers=2,collate_fn=collate_batch)\n",
        "dev_loader=DataLoader(dev_dataset,BATCH_SIZE,shuffle=False,num_workers=2,collate_fn=collate_batch)\n",
        "test_loader=DataLoader(test_dataset,BATCH_SIZE,shuffle=False,num_workers=2,collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "lstm_crf=LSTM_CRF(word2emb,train_dataset.dic_word,train_dataset.dic_char).cuda()\n",
        "optimizer=torch.optim.Adam(lstm_crf.parameters(),LR)\n",
        "\n",
        "best_score=0\n",
        "for epoch in range(num_epochs):\n",
        "    lstm_crf.train()\n",
        "    loss_lis=[]\n",
        "    pbar=tqdm(total=len(train_loader))\n",
        "    for i,(word_num,word_ids,char_ids,label_ids) in enumerate(train_loader):\n",
        "        loss=lstm_crf(word_num.cuda(),word_ids.cuda(),char_ids.cuda(),label_ids.cuda())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #nn.utils.clip_grad_norm_(lstm_crf.parameters(),CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_lis.append(loss.item())\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "    mean_loss=torch.mean(torch.tensor(loss_lis)).item()\n",
        "\n",
        "    lstm_crf.eval()\n",
        "    f1_score=0\n",
        "\n",
        "    #for loader in (dev_loader,test_loader):\n",
        "    loader=test_loader\n",
        "\n",
        "    outputs=[]\n",
        "    for i,(word_num,word_ids,char_ids,label_ids) in enumerate(loader):\n",
        "        word_num,word_ids,char_ids,label_ids=word_num.cuda(),word_ids.cuda(),char_ids.cuda(),label_ids.cuda()\n",
        "        pred=lstm_crf.decode(word_num,word_ids,char_ids)\n",
        "        outputs+=list_batch(pred,word_num,word_ids,label_ids, train_dataset.dic_word, train_dataset.label_map)\n",
        "\n",
        "    f=open('outputs.txt','w',encoding='utf-8')\n",
        "    f.write('\\n'.join(outputs))\n",
        "    f.close()\n",
        "    \n",
        "    os.system(\"python -m conlleval outputs.txt > results\")\n",
        "    #os.system(\"./conlleval < outputs.txt > results\")\n",
        "    f=open('results','r',encoding='utf-8')\n",
        "    f1_score=float(f.readlines()[1].split()[-1])\n",
        "    f.close()\n",
        "    best_score=max(best_score,f1_score)\n",
        "    \n",
        "    #print('epoch %d:  mean loss: %.4f  f1 score dev: %.2f  test: %.2f'%(epoch,mean_loss,f1_score[0],f1_score[1]))\n",
        "    print('epoch %d:  mean loss: %.4f  f1 score: %.2f  best: %.2f'%(epoch,mean_loss,f1_score,best_score))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CS598FIRSTDRAFTFinalProjet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3mxl0Z6GzTTBPcwURjT0G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}